{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PNQ89v5V-OPY",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Laptop\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:86: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2334/2334 - 513s - 220ms/step - accuracy: 0.5561 - loss: 1.0521\n",
      "Epoch 2/10\n",
      "2334/2334 - 482s - 207ms/step - accuracy: 0.6410 - loss: 0.8809\n",
      "Epoch 3/10\n",
      "2334/2334 - 463s - 198ms/step - accuracy: 0.6789 - loss: 0.7978\n",
      "Epoch 4/10\n",
      "2334/2334 - 448s - 192ms/step - accuracy: 0.7063 - loss: 0.7361\n",
      "Epoch 5/10\n",
      "2334/2334 - 487s - 209ms/step - accuracy: 0.7286 - loss: 0.6791\n",
      "Epoch 6/10\n",
      "2334/2334 - 570s - 244ms/step - accuracy: 0.7512 - loss: 0.6261\n",
      "Epoch 7/10\n",
      "2334/2334 - 600s - 257ms/step - accuracy: 0.7702 - loss: 0.5805\n",
      "Epoch 8/10\n",
      "2334/2334 - 579s - 248ms/step - accuracy: 0.7850 - loss: 0.5427\n",
      "Epoch 9/10\n",
      "2334/2334 - 588s - 252ms/step - accuracy: 0.7992 - loss: 0.5094\n",
      "Epoch 10/10\n",
      "2334/2334 - 595s - 255ms/step - accuracy: 0.8122 - loss: 0.4771\n",
      "\n",
      "Sentiment Prediction using LSTM\n",
      "\n",
      "Menu:\n",
      "1. Enter a review\n",
      "2. Exit\n",
      "Enter your choice (1/2): 1\n",
      "\n",
      "Enter a review: Check out this epic streamer!.\n",
      "1/1 - 1s - 1s/step\n",
      "Predicted Sentiment: Neutral\n",
      "\n",
      "Menu:\n",
      "1. Enter a review\n",
      "2. Exit\n",
      "Enter your choice (1/2): 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load and preprocess data\n",
    "data = pd.read_csv('twitter_training.csv', header=None)\n",
    "data.columns = ['Sr No', 'Ecommerce Website', 'Sentiment', 'Review']\n",
    "data = data[['Sentiment', 'Review']]\n",
    "\n",
    "# Text preprocessing\n",
    "data['Review'] = data['Review'].astype(str)  # Ensure all values are strings\n",
    "data['Review'] = data['Review'].str.lower()\n",
    "data['Review'] = data['Review'].str.replace(r'\\brt\\b', ' ', regex=True)\n",
    "data['Review'] = data['Review'].apply(lambda x: ''.join([char if char.isalnum() or char.isspace() else '' for char in x]))\n",
    "\n",
    "# Sentiment encoding\n",
    "sentiment_map = {'Positive': 0, 'Negative': 1, 'Neutral': 2, 'Irrelevant': 3}\n",
    "data['Sentiment'] = data['Sentiment'].map(sentiment_map)\n",
    "\n",
    "# Tokenization and padding\n",
    "max_features = 2000\n",
    "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
    "tokenizer.fit_on_texts(data['Review'].values)\n",
    "X = tokenizer.texts_to_sequences(data['Review'].values)\n",
    "X = pad_sequences(X)\n",
    "\n",
    "# Convert Sentiment to categorical\n",
    "Y = to_categorical(data['Sentiment'].values, num_classes=4)\n",
    "\n",
    "# Build model\n",
    "embed_dim = 128\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features, embed_dim, input_length=X.shape[1]))\n",
    "model.add(SpatialDropout1D(0.4))\n",
    "model.add(LSTM(196, dropout=0.2, recurrent_dropout=0.2))\n",
    "model.add(Dense(4, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.fit(X, Y, epochs=10, batch_size=32, verbose=2)\n",
    "\n",
    "print(\"\\nSentiment Prediction using LSTM\")\n",
    "maxlen = X.shape[1]\n",
    "sentiment_classes = ['Positive', 'Negative', 'Neutral', 'Irrelevant']\n",
    "\n",
    "while True:\n",
    "    \n",
    "    print(\"\\nMenu:\")\n",
    "    print(\"1. Enter a review\")\n",
    "    print(\"2. Exit\")\n",
    "    ch = input(\"Enter your choice (1/2): \")\n",
    "\n",
    "    if ch == '1':\n",
    "        ip = input(\"\\nEnter a review: \")\n",
    "        ip = ip.lower()\n",
    "        ip = ''.join([char if char.isalnum() or char.isspace() else '' for char in ip])\n",
    "        \n",
    "        review_seq = tokenizer.texts_to_sequences([ip])\n",
    "        review_seq = pad_sequences(review_seq, maxlen=maxlen, dtype='int32', value=0)\n",
    "        \n",
    "        sentiment = model.predict(review_seq, batch_size=1, verbose=2)[0]\n",
    "        print(f\"Predicted Sentiment: {sentiment_classes[np.argmax(sentiment)]}\")\n",
    "   \n",
    "    elif ch == '2':\n",
    "        break\n",
    "    else:\n",
    "        print(\"Invalid choice\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (2357671739.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[1], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    numpy: Library for numerical operations.\u001b[0m\n\u001b[1;37m                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# numpy: Library for numerical operations.\n",
    "# pandas: Used to handle and manipulate data in a tabular format (CSV files).\n",
    "# tensorflow.keras.preprocessing.text.Tokenizer: Tokenizer to convert text data into sequences (integer representation of words).\n",
    "# tensorflow.keras.preprocessing.sequence.pad_sequences: To ensure the sequences of text are of equal length by padding them.\n",
    "# tensorflow.keras.models.Sequential: Defines a linear stack of layers in the neural network.\n",
    "# tensorflow.keras.layers.Dense, Embedding, LSTM, SpatialDropout1D: Layers used in the LSTM-based model.\n",
    "# tensorflow.keras.utils.to_categorical: Converts labels to categorical format (one-hot encoded).\n",
    "\n",
    "    \n",
    "# Loading and Preprocessing the Data:\n",
    "# Loads the dataset from a CSV file (twitter_training.csv), where columns represent Sr No, Ecommerce Website, Sentiment, and Review.\n",
    "# The dataset is filtered to only include Sentiment and Review columns.\n",
    "\n",
    "\n",
    "# Text Preprocessing:\n",
    "# Converts the Review column to string format.\n",
    "# Converts all text to lowercase for uniformity.\n",
    "# Removes Twitter-specific artifacts like \"RT\" (retweets).\n",
    "# Removes all non-alphanumeric characters except spaces using a lambda function.\n",
    "\n",
    "# Sentiment Encoding:\n",
    "# Maps sentiment labels (Positive, Negative, Neutral, Irrelevant) to numeric values (0, 1, 2, 3).\n",
    "\n",
    "# Tokenization and Padding:\n",
    "# Tokenizer: Converts the text reviews into sequences of integers, where each word is mapped to a unique integer.\n",
    "# num_words=max_features: Limits the vocabulary to the top 2000 most frequent words.\n",
    "# pad_sequences: Pads sequences to ensure uniform length, which is crucial for feeding the data into an LSTM model.\n",
    " \n",
    "\n",
    "# Convert Sentiment to Categorical:    \n",
    "# Converts the sentiment labels (0, 1, 2, 3) into categorical (one-hot) format with 4 classes.\n",
    "\n",
    "\n",
    "# Building the LSTM Model:\n",
    "# Embedding Layer: Turns integer sequences into dense vector representations of fixed size (embed_dim=128).\n",
    "# SpatialDropout1D: Applies dropout to the input sequence, which helps in preventing overfitting.\n",
    "# LSTM Layer: Long Short-Term Memory (LSTM) network processes sequential data. It contains 196 units, with dropout for both input and recurrent connections (dropout=0.2, recurrent_dropout=0.2).\n",
    "# Dense Layer: A fully connected layer that outputs the prediction. The softmax activation function is used because this is a multi-class classification problem.\n",
    "# Model Compilation: Uses categorical_crossentropy as the loss function for multi-class classification and the adam optimizer.\n",
    "    \n",
    "    \n",
    "# Training the Model:\n",
    "# The model is trained on the input data (X) and target labels (Y) for 10 epochs with a batch size of 32.\n",
    "\n",
    "\n",
    "# Sentiment Prediction:\n",
    "# The user enters a review.\n",
    "# The review is preprocessed similarly to the training data (lowercased, non-alphanumeric characters removed).\n",
    "# The review is converted to a sequence of integers and padded to match the input length (maxlen).\n",
    "# The model predicts the sentiment, and the sentiment class with the highest probability is selected and displayed.\n",
    "\n",
    "\n",
    "\n",
    "# LSTM (Long Short-Term Memory) is a type of Recurrent Neural Network (RNN) specifically designed to address the issue of learning long-term dependencies. \n",
    "# RNNs have trouble retaining information over long sequences due to the vanishing gradient problem. \n",
    "# LSTMs solve this by introducing gates that control the flow of information, allowing them to \"remember\" important information over long sequences and \"forget\" irrelevant details.\n",
    "\n",
    "# Key Components of LSTM:\n",
    "# Cell State: The cell state is the \"memory\" of the network, which carries information \n",
    "# across time steps. The information in the cell state is updated by the gates at each time step.\n",
    "\n",
    "# Forget Gate: Decides what information from the previous cell state should be discarded.\n",
    "\n",
    "# Input Gate: Determines what new information should be stored in the cell state.\n",
    "\n",
    "# Output Gate: Decides what part of the cell state should be output as the hidden state to the next time step.\n",
    "\n",
    "# LSTM for Sentiment Analysis:\n",
    "# LSTM is ideal for sentiment analysis because reviews, tweets, or any text data are sequential, meaning the order of words matters.\n",
    "# LSTM networks capture the temporal dependencies in text by processing the sequence word by word, \n",
    "# making them effective for tasks like sentiment analysis, where the sentiment can depend on the context of words in a sentence.\n",
    "\n",
    "\n",
    "\n",
    "# Question: What is the role of the Embedding layer in this model?\n",
    "# Answer: The Embedding layer is used to convert the integer representation of words (obtained via tokenization) into dense vector representations (embeddings) of fixed size. \n",
    "# This helps capture semantic relationships between words, where similar words will have similar vector representations.\n",
    "\n",
    "# 2. Question: Why do we use SpatialDropout1D in this model?\n",
    "# Answer: SpatialDropout1D is used to drop entire 1D feature maps (rows of the input data) instead of individual features. This helps prevent overfitting by forcing the model to learn more robust features, improving generalization.\n",
    "\n",
    "# 3. Question: What does pad_sequences do in the code?\n",
    "# Answer: pad_sequences ensures that all input sequences (reviews) are of the same length by adding padding (usually zeros) to the sequences that are shorter than the defined length, and truncating those that are longer.\n",
    "\n",
    "# 4. Question: What is the purpose of using to_categorical for the sentiment labels?\n",
    "# Answer: to_categorical converts the sentiment labels into one-hot encoded vectors, which are required for multi-class classification. \n",
    "# Each label is converted into a vector of length 4, where the corresponding index for the sentiment class is set to 1, and the rest are 0.\n",
    "\n",
    "# 5. Question: Why is softmax used in the output layer?\n",
    "# Answer: Softmax is used in the output layer because it is a multi-class classification problem. \n",
    "# Softmax converts the output into probabilities, where the class with the highest probability is considered the predicted sentiment.\n",
    "\n",
    "# 6. Question: How does LSTM handle long-term dependencies in text?\n",
    "# Answer: LSTM networks use a memory cell and gates (forget, input, and output gates) to store, update, and output information over long sequences, allowing them to retain important information and discard irrelevant parts, thus handling long-term dependencies effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
